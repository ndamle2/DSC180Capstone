{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7975971-902f-4723-b6e7-a9335849d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import networkx as nx\n",
    "import gzip\n",
    "import json\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ccdedd-9011-4007-83e0-4dc724b315c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"xbar/1/xbar.json.gz\", 'rb') as f:\n",
    "    instances = json.loads(f.read().decode('utf-8'))['instances']\n",
    "\n",
    "with gzip.open(\"cells.json.gz\", 'rb') as f:\n",
    "    cells = json.loads(f.read().decode('utf-8'))\n",
    "\n",
    "conn = np.load(\"xbar/1/xbar_connectivity.npz\")\n",
    "coo = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "adj_matrix = (np.dot(coo.toarray(), coo.toarray().T) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928980c4-20d2-481d-8be9-5a32ebcddbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGRCIndex(xloc, yloc, xBoundaryList, yBoundaryList):\n",
    "    \"\"\"\n",
    "    Get the GRC index for a given x, y location.\n",
    "    Args:\n",
    "        xloc (int): X-coordinate in database units.\n",
    "        yloc (int): Y-coordinate in database units.\n",
    "        xBoundaryList (np.ndarray): Array of x-boundaries for GRCs.\n",
    "        yBoundaryList (np.ndarray): Array of y-boundaries for GRCs.\n",
    "    Returns:\n",
    "        tuple: (i, j) indices of the GRC in the grid.\n",
    "    \"\"\"\n",
    "    # Find the GRC index for xloc and yloc\n",
    "    i = np.searchsorted(yBoundaryList, yloc, side='right') - 1\n",
    "    j = np.searchsorted(xBoundaryList, xloc, side='right') - 1\n",
    "    return i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4228d42-2098-4385-8111-616b42564b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('xbar/1/xbar_congestion.npz')\n",
    "\n",
    "# Get the index for layer M1\n",
    "lyr = list(data['layerList']).index('M1')\n",
    "\n",
    "# Get boundary arrays for GRCs\n",
    "ybl = data['yBoundaryList']  # y-coordinate boundaries\n",
    "xbl = data['xBoundaryList']  # x-coordinate boundaries\n",
    "\n",
    "for instance in instances:\n",
    "    xloc, yloc = instance['xloc'], instance['yloc']\n",
    "    i, j = getGRCIndex(xloc, yloc, xbl, ybl)  # Compute GRC indices\n",
    "\n",
    "    # Retrieve demand and capacity\n",
    "    demand = data['demand'][lyr][i][j]\n",
    "    capacity = data['capacity'][lyr][i][j]\n",
    "    congestion = demand / capacity if capacity > 0 else demand  # Calculate congestion\n",
    "\n",
    "    # Add congestion data as a feature\n",
    "    instance['demand'] = demand\n",
    "    instance['capacity'] = capacity\n",
    "    instance['congestion'] = congestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d25001f-41b0-42a8-b2e2-36d778c567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_virtual_nodes(adj, num_nodes, partition_k=4):\n",
    "    num_vns = partition_k + 1  # partition_k first-level VNs + 1 super-VN\n",
    "    new_size = num_nodes + num_vns\n",
    "    \n",
    "    # Expand adjacency matrix\n",
    "    new_adj = np.zeros((new_size, new_size), dtype=int)\n",
    "    new_adj[:num_nodes, :num_nodes] = adj  # Copy the original adjacency matrix\n",
    "\n",
    "    # Partition nodes\n",
    "    partition_size = num_nodes // partition_k\n",
    "    partitions = [list(range(i * partition_size, (i + 1) * partition_size)) for i in range(partition_k)]\n",
    "\n",
    "    # Add first-level VNs\n",
    "    for i, part in enumerate(partitions):\n",
    "        vn_idx = num_nodes + i\n",
    "        for node in part:\n",
    "            new_adj[node, vn_idx] = 1\n",
    "            new_adj[vn_idx, node] = 1\n",
    "\n",
    "    # Add super-VN\n",
    "    super_vn_idx = num_nodes + partition_k\n",
    "    for i in range(partition_k):\n",
    "        vn_idx = num_nodes + i\n",
    "        new_adj[vn_idx, super_vn_idx] = 1\n",
    "        new_adj[super_vn_idx, vn_idx] = 1\n",
    "\n",
    "    return new_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dca0a80-4c3b-41fa-bbe1-5a6e64336b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(adj_with_vns, instances):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch Geometric Data object with features based on instances.\n",
    "\n",
    "    Args:\n",
    "        adj_with_vns (np.ndarray): Adjacency matrix including virtual nodes.\n",
    "        instances (list of dict): List of node attributes, each corresponding to an original node.\n",
    "\n",
    "    Returns:\n",
    "        Data: PyTorch Geometric Data object with features and edge index.\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor(np.array(np.nonzero(adj_with_vns)), dtype=torch.long)\n",
    "\n",
    "    # Extract number of original and virtual nodes\n",
    "    num_nodes = adj_with_vns.shape[0]\n",
    "    num_original_nodes = len(instances)\n",
    "    num_virtual_nodes = num_nodes - num_original_nodes\n",
    "\n",
    "    # Create feature matrix (exclude demand and capacity from features)\n",
    "    features = []\n",
    "    for instance in instances:\n",
    "        # Extract relevant fields as features (excluding 'demand' and 'capacity')\n",
    "        features.append([\n",
    "            instance['xloc'],        # x-coordinate\n",
    "            instance['yloc'],        # y-coordinate\n",
    "            instance['cell'],        # Cell type\n",
    "            instance['orient'],      # Orientation\n",
    "            #instance['congestion'],  # Congestion (calculated earlier)\n",
    "        ])\n",
    "    \n",
    "    # Add dummy features for virtual nodes (or zero features as placeholders)\n",
    "    for _ in range(num_virtual_nodes):\n",
    "        features.append([0, 0, 0, 0])  # Example: zeros for virtual nodes\n",
    "\n",
    "    # Convert features to a tensor\n",
    "    features = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "    # Create Data object\n",
    "    data = Data(x=features, edge_index=edge_index)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988e39a3-1a5f-4d3a-b7f3-071e33b8f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDEHNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(BaseDEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.node_mlps = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim), \n",
    "            nn.ReLU()) for i in range(num_layers)])\n",
    "        self.net_mlps = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU()) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, edge_index, adj_matrix):\n",
    "        for i in range(self.num_layers):\n",
    "            # Node Update: Aggregate from neighbors (adjacency matrix)\n",
    "            net_features = torch.mm(adj_matrix, x)  # Shape: [N, input_dim] -> [N, 7]\n",
    "\n",
    "            x = x + self.node_mlps[i](net_features)  # The output should now have shape [N, hidden_dim]\n",
    "            \n",
    "            # Net Update: Aggregate from nodes to hyperedges (transpose adj matrix)\n",
    "            node_features = torch.mm(adj_matrix.T, x)  # Shape: [N, hidden_dim]\n",
    "            \n",
    "            # Apply the MLP on net features (Shape: [N, hidden_dim])\n",
    "            x = x + self.net_mlps[i](node_features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8adc5b-5f10-41e1-9bab-485e9b764c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(predicted, target):\n",
    "    return torch.sqrt(torch.mean((predicted - target) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0aeaaf-4256-41c4-aeaf-84b4dc28e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_virtual_node_instances(instances, num_virtual_nodes):\n",
    "    \"\"\"\n",
    "    Add dummy instances for virtual nodes to the instances list.\n",
    "    \n",
    "    Args:\n",
    "        instances (list): List of original instances (original node data).\n",
    "        num_virtual_nodes (int): The number of virtual nodes to add.\n",
    "        num_original_nodes (int): The number of original nodes.\n",
    "    \n",
    "    Returns:\n",
    "        list: The updated list of instances, including dummy instances for virtual nodes.\n",
    "    \"\"\"\n",
    "    # Create dummy instances for virtual nodes\n",
    "    for i in range(num_virtual_nodes):\n",
    "        dummy_instance = {\n",
    "            'xloc': 0,               # Placeholder x-coordinate\n",
    "            'yloc': 0,               # Placeholder y-coordinate\n",
    "            'cell': -1,       # Placeholder for cell type (can use any value)\n",
    "            'orient': 0,        # Placeholder for orientation (can use any value)\n",
    "            'demand': 0,             # Placeholder demand (assuming 0 demand for virtual nodes)\n",
    "            'capacity': 0,           # Placeholder capacity (assuming 0 capacity for virtual nodes)\n",
    "            'congestion': 0          # Placeholder congestion (assuming 0 for virtual nodes)\n",
    "        }\n",
    "        instances.append(dummy_instance)\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b43471d-4075-4b7e-95cc-6d984720723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3957) must match the size of tensor b (2960) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_data\u001b[38;5;241m.\u001b[39mx, train_data\u001b[38;5;241m.\u001b[39medge_index, adj_with_vns)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute loss using RMSE\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m rmse_loss(output\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m), train_demand)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mrmse_loss\u001b[0;34m(predicted, target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrmse_loss\u001b[39m(predicted, target):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mmean((predicted \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3957) must match the size of tensor b (2960) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# 4-fold cross-validation setup\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# To store RMSE results for each fold\n",
    "rmse_results = []\n",
    "\n",
    "# Prepare the feature tensor and target tensor\n",
    "features = torch.tensor([[inst['xloc'], inst['yloc'], inst['cell'], inst['orient']] for inst in instances], dtype=torch.float32)\n",
    "demand_tensor = torch.tensor([inst['demand'] for inst in instances], dtype=torch.float32)\n",
    "capacity_tensor = torch.tensor([inst['capacity'] for inst in instances], dtype=torch.float32)\n",
    "\n",
    "# Create adjacency matrix and graph data\n",
    "partition_k = 4\n",
    "adj_with_vns = add_virtual_nodes(adj_matrix, len(instances),partition_k=partition_k)  # Adjusted to len(instances)\n",
    "data = create_graph_data(adj_with_vns, instances)\n",
    "instances_with_vns = add_virtual_node_instances(instances, partition_k+1)\n",
    "adj_with_vns = torch.tensor(adj_with_vns, dtype=torch.float32)\n",
    "\n",
    "# Define model and optimizer\n",
    "model = BaseDEHNN(input_dim=4, hidden_dim=4, num_layers=3)  # Reduced input_dim because demand is no longer included in features\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(features[:-partition_k-1])):\n",
    "    print(f\"Fold {fold + 1}/{kf.get_n_splits()}\")\n",
    "\n",
    "    # Prepare training and validation data\n",
    "    train_features, val_features = features[train_idx], features[val_idx]\n",
    "    train_demand, val_demand = demand_tensor[train_idx], demand_tensor[val_idx]\n",
    "    train_capacity, val_capacity = capacity_tensor[train_idx], capacity_tensor[val_idx]\n",
    "\n",
    "    # Create training and validation graphs\n",
    "    train_data = create_graph_data(adj_with_vns[train_idx, train_idx], instances_with_vns) \n",
    "    print(train_data.shape)\n",
    "    val_data = create_graph_data(adj_with_vns[val_idx, val_idx], instances_with_vns)\n",
    "    print(val_data.shape)\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on training data\n",
    "        output = model(train_data.x, train_data.edge_index, adj_with_vns)\n",
    "\n",
    "        # Compute loss using RMSE\n",
    "        loss = rmse_loss(output.mean(1), train_demand)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(val_data.x, val_data.edge_index, adj_with_vns)\n",
    "        val_rmse = rmse_loss(output.mean(1), val_demand).item()\n",
    "        print(f\"Validation RMSE for fold {fold + 1}: {val_rmse}\")\n",
    "\n",
    "    # Store RMSE for each fold\n",
    "    rmse_results.append(val_rmse)\n",
    "\n",
    "# Compute average RMSE over all folds\n",
    "average_rmse = sum(rmse_results) / len(rmse_results)\n",
    "print(f\"Average RMSE across all folds: {average_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1160ae-a654-46a9-9417-0ec24b7b4435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
