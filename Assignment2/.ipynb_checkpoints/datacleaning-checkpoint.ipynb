{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5813ec-20f6-4960-bc50-cdc947c5352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import time\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50529c9e-09ff-47aa-84e3-6a88aad4b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "abs_dir = '/data/zluo/'\n",
    "raw_data_dir = abs_dir + 'new_data/'\n",
    "\n",
    "designs_list = [\n",
    "    'superblue1',\n",
    "    'superblue2',\n",
    "    'superblue3',\n",
    "    'superblue4',\n",
    "    'superblue5',\n",
    "    'superblue6',\n",
    "    'superblue7',\n",
    "    'superblue18',\n",
    "    'superblue19',\n",
    "    'superblue9',\n",
    "    'superblue11',\n",
    "    'superblue14',\n",
    "    'superblue16'\n",
    "]\n",
    "num_designs = len(designs_list)\n",
    "num_variants_list = [\n",
    "    5,\n",
    "    5,\n",
    "    6,\n",
    "    5,\n",
    "    6,\n",
    "    6,\n",
    "    6,\n",
    "    5,\n",
    "    6,\n",
    "    6,\n",
    "    6,\n",
    "    6,\n",
    "    6\n",
    "]\n",
    "\n",
    "# Generate all names\n",
    "sample_names = []\n",
    "corresponding_design = []\n",
    "corresponding_variant = []\n",
    "for idx in range(num_designs):\n",
    "    for variant in range(num_variants_list[idx]):\n",
    "        sample_name = raw_data_dir + designs_list[idx] + '/' + str(variant + 1) + '/'\n",
    "        sample_names.append(sample_name)\n",
    "        corresponding_design.append(designs_list[idx])\n",
    "        corresponding_variant.append(variant + 1)\n",
    "\n",
    "# Synthetic data\n",
    "N = len(sample_names)\n",
    "print(N)\n",
    "data_dir = '2023-03-06_data/'\n",
    "\n",
    "# +--------------------+\n",
    "# | Global information |\n",
    "# +--------------------+\n",
    "\n",
    "# Read the csv file\n",
    "with open(raw_data_dir + '/settings.csv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    words = line.strip().split(',')\n",
    "    dictionary = {\n",
    "        'design': words[0],\n",
    "        'variant': int(words[1]),\n",
    "        'core_utilization': float(words[2]),\n",
    "        'max_routing_layer': words[3],\n",
    "        'clk_per': int(words[4]),\n",
    "        'clk_uncertainty': float(words[5]),\n",
    "        'flow_stage': words[6],\n",
    "        'hstrap_layer': words[7],\n",
    "        'hstrap_width': float(words[8]),\n",
    "        'hstrap_pitch': float(words[9]),\n",
    "        'vstrap_layer': words[10],\n",
    "        'vstrap_width': float(words[11]),\n",
    "        'vstrap_pitch': float(words[12])\n",
    "    }\n",
    "\n",
    "    sample_idx = -1\n",
    "    for idx in range(N):\n",
    "        if dictionary['design'] == corresponding_design[idx] and dictionary['variant'] == corresponding_variant[idx]:\n",
    "            sample_idx = idx\n",
    "            break\n",
    "\n",
    "    fn = data_dir + str(sample_idx) + '.global_information.pkl'\n",
    "    f = open(fn, \"wb\")\n",
    "    pickle.dump(dictionary, f)\n",
    "    f.close()\n",
    "    print('Save file', fn)\n",
    "\n",
    "# +-------------------------+\n",
    "# | Information about cells |\n",
    "# +-------------------------+\n",
    "\n",
    "cells_fn = raw_data_dir + 'cells.json.gz'\n",
    "with gzip.open(cells_fn, 'r') as fin:\n",
    "    cell_data = json.load(fin)\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "for idx in range(len(cell_data)):\n",
    "    width = cell_data[idx]['width']\n",
    "    height = cell_data[idx]['height']\n",
    "    widths.append(width)\n",
    "    heights.append(height)\n",
    "\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "min_cell_width = np.min(widths)\n",
    "max_cell_width = np.max(widths)\n",
    "min_cell_height = np.min(heights)\n",
    "max_cell_height = np.max(heights)\n",
    "\n",
    "print('min cell width:', min_cell_width)\n",
    "print('max cell width:', max_cell_width)\n",
    "print('mean cell width:', np.mean(widths))\n",
    "print('std cell width:', np.std(widths))\n",
    "print()\n",
    "print('min cell height:', min_cell_height)\n",
    "print('max cell height:', max_cell_height)\n",
    "print('mean cell height:', np.mean(heights))\n",
    "print('std cell height:', np.std(heights))\n",
    "\n",
    "widths = (widths - min_cell_width) / (max_cell_width - min_cell_width)\n",
    "heights = (heights - min_cell_height) / (max_cell_height - min_cell_height)\n",
    "print('Done processing cell sizes')\n",
    "\n",
    "cell_to_edge_dict = {item['id']:{inner_item['id']: inner_item['dir'] for inner_item in item['terms']} for item in cell_data}\n",
    "print('Done processing edge types dict')\n",
    "\n",
    "# For each sample\n",
    "for sample in range(50, 74):\n",
    "    \n",
    "    # +--------------------------------------+\n",
    "    # | Information about instances and nets |\n",
    "    # +--------------------------------------+\n",
    "\n",
    "    folder = sample_names[sample]\n",
    "    design = corresponding_design[sample]\n",
    "    instances_nets_fn = folder + design + '.json.gz'\n",
    "\n",
    "    print('--------------------------------------------------')\n",
    "    print('Folder:', folder)\n",
    "    print('Design:', design)\n",
    "    print('Instances & nets info:', instances_nets_fn)\n",
    "\n",
    "    with gzip.open(instances_nets_fn, 'r') as fin:\n",
    "        instances_nets_data = json.load(fin)\n",
    "\n",
    "    instances = instances_nets_data['instances']\n",
    "    nets = instances_nets_data['nets']\n",
    "\n",
    "    inst_to_cell = {item['id']:item['cell'] for item in instances}\n",
    "\n",
    "    num_instances = len(instances)\n",
    "    num_nets = len(nets)\n",
    "\n",
    "    print('Number of instances:', num_instances)\n",
    "    print('Number of nets:', num_nets)\n",
    "    \n",
    "    # Get placements info\n",
    "    xloc_list = [instances[idx]['xloc'] for idx in range(num_instances)]\n",
    "    yloc_list = [instances[idx]['yloc'] for idx in range(num_instances)]\n",
    "    cell = [instances[idx]['cell'] for idx in range(num_instances)]\n",
    "    cell_width = [widths[cell[idx]] for idx in range(num_instances)]\n",
    "    cell_height = [heights[cell[idx]] for idx in range(num_instances)]\n",
    "    orient = [instances[idx]['orient'] for idx in range(num_instances)]\n",
    "    \n",
    "    x_min = min(xloc_list)\n",
    "    x_max = max(xloc_list)\n",
    "    y_min = min(yloc_list)\n",
    "    y_max = max(yloc_list)\n",
    "\n",
    "    print('min xloc:', x_min)\n",
    "    print('max xloc:', x_max)\n",
    "    print('min yloc:', y_min)\n",
    "    print('max yloc:', y_max)\n",
    "\n",
    "    X = np.expand_dims(np.array(xloc_list), axis = 1)\n",
    "    Y = np.expand_dims(np.array(yloc_list), axis = 1)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    Y = (Y - y_min) / (y_max - y_min)\n",
    "\n",
    "    cell = np.expand_dims(np.array(cell), axis = 1)\n",
    "    cell_width = np.expand_dims(np.array(cell_width), axis = 1)\n",
    "    cell_height = np.expand_dims(np.array(cell_height), axis = 1)\n",
    "    orient = np.expand_dims(np.array(orient), axis = 1)\n",
    "\n",
    "    instance_features = np.concatenate((X, Y, cell, cell_width, cell_height, orient), axis = 1)\n",
    "    \n",
    "    dictionary = {\n",
    "        'num_instances': num_instances,\n",
    "        'num_nets': num_nets,\n",
    "        'x_min': x_min,\n",
    "        'x_max': x_max,\n",
    "        'y_min': y_min,\n",
    "        'y_max': y_max,\n",
    "        'min_cell_width': min_cell_width,\n",
    "        'max_cell_width': max_cell_width,\n",
    "        'min_cell_height': min_cell_height,\n",
    "        'max_cell_height': max_cell_height,\n",
    "        'instance_features': instance_features,\n",
    "        'sample_name': sample_name,\n",
    "        'folder': folder,\n",
    "        'design': design\n",
    "    }\n",
    "    fn = data_dir + '/' + str(sample) + '.node_features.pkl'\n",
    "    f = open(fn, \"wb\")\n",
    "    pickle.dump(dictionary, f)\n",
    "    f.close()\n",
    "    print('Save file', fn)\n",
    "\n",
    "    # +-------------------------+\n",
    "    # | Get the connection data |\n",
    "    # +-------------------------+\n",
    "\n",
    "    connection_fn = folder + design + '_connectivity.npz'\n",
    "    connection_data = np.load(connection_fn)\n",
    "    print('Connection info:', connection_fn)\n",
    "    \n",
    "    # get the direction of each edge between inst and net\n",
    "    dirs = []\n",
    "    edge_t = connection_data['data']\n",
    "    instance_idx = connection_data['row']\n",
    "    \n",
    "    for idx in range(len(instance_idx)):\n",
    "        inst = instance_idx[idx]\n",
    "        cell = inst_to_cell[inst]\n",
    "        edge_dict = cell_to_edge_dict[cell]\n",
    "        t = edge_t[idx]\n",
    "        direction = edge_dict[t]\n",
    "        dirs.append(direction)\n",
    "\n",
    "    dirs = np.array(dirs)\n",
    "    \n",
    "    assert dirs.shape == connection_data['data'].shape\n",
    "    \n",
    "    dictionary = {\n",
    "        'instance_idx': connection_data['row'],\n",
    "        'net_idx': connection_data['col'],\n",
    "        'edge_attr': connection_data['data'],\n",
    "        'edge_dir': dirs, \n",
    "        'sample_name': sample_name,\n",
    "        'folder': folder,\n",
    "        'design': design\n",
    "    }\n",
    "    \n",
    "    edge_index = np.array([dictionary['instance_idx'], dictionary['net_idx']]).T\n",
    "    edge_dir = dictionary['edge_dir']\n",
    "    n_edge_index = []\n",
    "    for idx in range(len(edge_index)):\n",
    "        tp = edge_index[idx]\n",
    "        direct = edge_dir[idx]\n",
    "\n",
    "        if direct == 0:\n",
    "            n_edge_index.append([tp[1], tp[0]])\n",
    "        else:\n",
    "            n_edge_index.append([tp[0], tp[1]])\n",
    "\n",
    "    n_edge_index = np.array(n_edge_index).T\n",
    "   \n",
    "    dictionary['edge_index'] = n_edge_index\n",
    "\n",
    "    \n",
    "    fn = data_dir + '/' + str(sample) + '.bipartite.pkl'\n",
    "    f = open(fn, \"wb\")\n",
    "    pickle.dump(dictionary, f)\n",
    "    f.close()\n",
    "    print('Save file', fn)\n",
    "    \n",
    "    \n",
    "    instances = dictionary['instance_idx']\n",
    "    nets = dictionary['net_idx']\n",
    "    directs = dictionary['edge_dir']\n",
    "    attrs = dictionary['edge_attr']\n",
    "    \n",
    "    drive_dict = dict()\n",
    "    for idx in range(len(instances)):\n",
    "        direct = directs[idx]\n",
    "        net = nets[idx]\n",
    "\n",
    "        if direct == 1:\n",
    "            assert net not in drive_dict\n",
    "\n",
    "            drive_dict[net] = idx\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    print(\"finished building drive and net dict\")\n",
    "\n",
    "\n",
    "    n_row = []\n",
    "    n_col = []\n",
    "    n_edge_attr = []\n",
    "    b_nets = []\n",
    "    b_terms = []\n",
    "    for idx in range(len(nets)):\n",
    "        net = nets[idx]\n",
    "        inst = instances[idx]\n",
    "        attr = attrs[idx]\n",
    "        \n",
    "        if net not in drive_dict:\n",
    "            b_nets.append(net)\n",
    "            b_terms.append(idx)\n",
    "        else:\n",
    "            inst_idx = drive_dict[net]\n",
    "            drive_inst = instances[inst_idx]\n",
    "            \n",
    "            if inst == drive_inst:\n",
    "                continue\n",
    "                \n",
    "            n_row.append(drive_inst)\n",
    "            n_col.append(inst)\n",
    "            n_edge_attr.append(attr)\n",
    "    \n",
    "    \n",
    "    #dictionary['edge_attr'] = np.array(n_edge_attr)\n",
    "    edge_index = np.array([n_row, n_col]).T\n",
    "    dictionary['edge_index'] = edge_index\n",
    "    dictionary['b_nets'] = b_nets\n",
    "    dictionary['b_terms'] = b_terms\n",
    "    \n",
    "    \n",
    "    \n",
    "    pos_lst = instance_features[:, :2] \n",
    "    for idx in range(len(edge_index)):\n",
    "        tp = edge_index[idx]\n",
    "        direct = edge_dir[idx]\n",
    "\n",
    "        # compute the edge distances based on nodes' positions\n",
    "        first_pos = pos_lst[tp[0]]\n",
    "        second_pos = pos_lst[tp[1]]\n",
    "        l1_dis = np.linalg.norm((first_pos - second_pos), ord=1)\n",
    "        new_attr = [l1_dis]\n",
    "        if idx == 0:\n",
    "            print(new_attr)\n",
    "        n_edge_attr[idx] = new_attr\n",
    "    \n",
    "\n",
    "    n_edge_attr = np.array(n_edge_attr)\n",
    "    print(n_edge_attr.shape, n_edge_attr[-1])\n",
    "    dictionary['edge_attr'] = n_edge_attr\n",
    "    \n",
    "\n",
    "    fn = data_dir + '/' + str(sample) + '.star.pkl'\n",
    "    f = open(fn, \"wb\")\n",
    "    pickle.dump(dictionary, f)\n",
    "    f.close()\n",
    "    print('Save file', fn)\n",
    "\n",
    "\n",
    "    # +---------------------+\n",
    "    # | Get congestion data |\n",
    "    # +---------------------+\n",
    "\n",
    "    congestion_fn = folder + design + '_congestion.npz'\n",
    "    congestion_data = np.load(congestion_fn)\n",
    "    print('Congestion info:', congestion_fn)\n",
    "\n",
    "    congestion_data_demand = congestion_data['demand']\n",
    "    congestion_data_capacity = congestion_data['capacity']\n",
    "\n",
    "    num_layers = len(list(congestion_data['layerList']))\n",
    "    print('Number of layers:', num_layers)\n",
    "    print('Layers:', list(congestion_data['layerList']))\n",
    "\n",
    "    ybl = congestion_data['yBoundaryList']\n",
    "    xbl = congestion_data['xBoundaryList']\n",
    "\n",
    "    all_demand = []\n",
    "    all_capacity = []\n",
    "\n",
    "    for layer in list(congestion_data['layerList']):\n",
    "        print('Layer', layer, ':')\n",
    "        lyr = list(congestion_data['layerList']).index(layer)\n",
    "\n",
    "        # Binned statistics 2D\n",
    "        t = time.time()\n",
    "        ret = binned_statistic_2d(xloc_list, yloc_list, None, 'count', bins = [xbl[1:], ybl[1:]], expand_binnumbers = True)\n",
    "        print('Time for binned statistics:', time.time() - t)\n",
    "\n",
    "        i_list = np.array([ret.binnumber[0, idx] - 1 for idx in range(num_instances)])\n",
    "        j_list = np.array([ret.binnumber[1, idx] - 1 for idx in range(num_instances)])\n",
    "\n",
    "        # Get demand and capacity\n",
    "        t = time.time()\n",
    "        demand_list = congestion_data_demand[lyr, i_list, j_list].flatten()\n",
    "        capacity_list = congestion_data_capacity[lyr, i_list, j_list].flatten()\n",
    "        print('Time to get demand and capacity:', time.time() - t)\n",
    "\n",
    "        demand_list = np.array(demand_list)\n",
    "        capacity_list = np.array(capacity_list)\n",
    "\n",
    "        all_demand.append(np.expand_dims(demand_list, axis = 1))\n",
    "        all_capacity.append(np.expand_dims(capacity_list, axis = 1))\n",
    "\n",
    "        average_demand = np.mean(demand_list)\n",
    "        average_capacity = np.mean(capacity_list)\n",
    "        average_diff = np.mean(capacity_list - demand_list)\n",
    "        count_congestions = np.sum(demand_list > capacity_list)\n",
    "\n",
    "        print('    Number of demand > capacity:', count_congestions)\n",
    "        print('    Average capacity - demand:', average_diff)\n",
    "        print('    Average demand:', average_demand)\n",
    "        print('    Average capacity:', average_capacity)\n",
    "\n",
    "    demand = np.concatenate(all_demand, axis = 1)\n",
    "    capacity = np.concatenate(all_capacity, axis = 1)\n",
    "\n",
    "    dictionary = {\n",
    "        'demand': demand,\n",
    "        'capacity': capacity\n",
    "    }\n",
    "    fn = data_dir + '/' + str(sample) + '.targets.pkl'\n",
    "    f = open(fn, \"wb\")\n",
    "    pickle.dump(dictionary, f)\n",
    "    f.close()\n",
    "    print('Save file', fn)\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
